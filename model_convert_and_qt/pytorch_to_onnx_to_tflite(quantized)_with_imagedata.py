# -*- coding: utf-8 -*-
"""pytorch_to_onnx_to_tflite(quantized)_with_imagedata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/nyadla-sys/tflite-micro/blob/pytorch_to_tflite_conversion/third_party/xtensa/examples/pytorch_to_tflite/pytorch_to_tflite_converter/pytorch_to_onnx_to_tflite_int8.ipynb

## Install ONNX and ONNX runtime
"""

# Some standard imports
import numpy as np
import torch
import torch.onnx
import torchvision.models as models
import onnx
import onnxruntime
import torch_pruning as tp
"""## Load mobilenetV2 from torch models"""

from model import Model



model = Model()

model.eval()


'''##Prune the model'''
dep_graph = tp.DependencyGraph().build_dependency( model, torch.randn(1, 3, 224, 224) )
group = dep_graph.get_pruning_group(model.features[0][0],tp.prune_conv_out_channels,idxs=[0,1,2,3,4,5])

if dep_graph.check_pruning_group(group):
   group.prune()



"""##Convert from pytorch to onnx"""

IMAGE_SIZE = 224
BATCH_SIZE = 1
IMAGE_SIZE = 224
# Input to the model
x = torch.randn(BATCH_SIZE, 1, 28, 28, requires_grad=True)
torch_out = model(x)




# Export the model
torch.onnx.export(model,                     # model being run
                  x,                         # model input (or a tuple for multiple inputs)
                  "mobilenet_v2.onnx",       # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=10,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = ['input'],   # the model's input names
                  output_names = ['output'], # the model's output names
                  dynamic_axes={'input' : {0 : 'BATCH_SIZE'},    # variable length axes
                                'output' : {0 : 'BATCH_SIZE'}})

onnx_model = onnx.load("mobilenet_v2.onnx")
onnx.checker.check_model(onnx_model)

"""##Compare ONNX Runtime and Pytorch results"""

ort_session = onnxruntime.InferenceSession("mobilenet_v2.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

print("Exported model has been tested with ONNXRuntime, and the result looks good!")

"""##Convert from Onnx to TF saved model"""


from onnx_tf.backend import prepare
import onnx

onnx_model_path = 'mobilenet_v2.onnx'
tf_model_path = 'model_tf'

onnx_model = onnx.load(onnx_model_path)
tf_rep = prepare(onnx_model)
tf_rep.export_graph(tf_model_path)

"""##Convert from TF saved model to TFLite(float32) model"""

import tensorflow as tf

saved_model_dir = 'model_tf'
tflite_model_path = 'mobilenet_v2_float32.tflite'

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

# Save the model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

"""##Run inference on TFLite(float32) with random data"""

import numpy as np
import tensorflow as tf

tflite_model_path = './mobilenet_v2_float32.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data
input_shape = input_details[0]['shape']
print(input_shape)
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# get_tensor() returns a copy of the tensor data
# use tensor() in order to get a pointer to the tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

print("Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%"
      .format(np.argmax(output_data),
              100 * output_data[0][np.argmax(output_data)]))

"""##Run inference on TFLite(float32) with image data"""

import os
import zipfile

local_zip = './cats_and_dogs_filtered.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('.')
zip_ref.close()

import tensorflow as tf
import numpy as np
tflite_model_path = './mobilenet_v2_float32.tflite'

#tflite_model_path = './model_float32.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

print("== Input details ==")
print("name:", interpreter.get_input_details()[0]['name'])
print("shape:", interpreter.get_input_details()[0]['shape'])
print("type:", interpreter.get_input_details()[0]['dtype'])

print("\nDUMP INPUT")
print(interpreter.get_input_details()[0])

print("\n== Output details ==")
print("name:", interpreter.get_output_details()[0]['name'])
print("shape:", interpreter.get_output_details()[0]['shape'])
print("type:", interpreter.get_output_details()[0]['dtype'])

print("\nDUMP OUTPUT")
print(interpreter.get_output_details()[0])

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on image  data
input_shape = input_details[0]['shape']
#print(input_shape)
image = tf.io.read_file('./cats_and_dogs_filtered/validation/cats/cat.2000.jpg')

image = tf.io.decode_jpeg(image, channels=3)
image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])
image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])
image = tf.expand_dims(image, 0)
print("Real image shape")
print(image.shape)
#print(image)
interpreter.set_tensor(input_details[0]['index'], image)

interpreter.invoke()

# get_tensor() returns a copy of the tensor data
# use tensor() in order to get a pointer to the tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

print("Predicted value . Label index: {}, confidence: {:2.0f}%"
      .format(np.argmax(output_data),
              100 * output_data[0][np.argmax(output_data)]))

"""####Convert from TF saved model to TFLite(quantized) model"""

# A generator that provides a representative dataset
import tensorflow as tf
from PIL import Image
from torchvision import transforms
saved_model_dir = 'model_tf'
#flowers_dir = './images'
def representative_data_gen():
  dataset_list = tf.data.Dataset.list_files('./cats_and_dogs_filtered/train' + '/*/*')
  for i in range(1):
    image = next(iter(dataset_list))
    image = tf.io.read_file(image)
    image = tf.io.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])
    image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])
    image = tf.cast(image / 127., tf.float32)
    image = tf.expand_dims(image, 0)
    print(image.shape)
    yield [image]

from PIL import Image
from torchvision import transforms
# Download an example image from the pytorch website
import urllib
url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
try: urllib.URLopener().retrieve(url, filename)
except: urllib.request.urlretrieve(url, filename)

def representative_data_gen_1():
  dataset_list = tf.data.Dataset.list_files('./cats_and_dogs_filtered/train' + '/*/*')
  for i in range(100):
    input_image = next(iter(dataset_list))
    input_image = Image.open(filename)
    preprocess = transforms.Compose([
    transforms.RandomCrop(224, padding=4),
    transforms.Resize(224),
    transforms.RandomHorizontalFlip(),
    #transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    input_tensor = preprocess(input_image)
    print(input_tensor.shape)
    input_tensor = tf.expand_dims(input_tensor, 0)
    print("torch input_tensor size")
    print(input_tensor.shape)
    yield [input_tensor]

#converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
# This enables quantization
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# This sets the representative dataset for quantization
converter.representative_dataset = representative_data_gen_1
# This ensures that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.
converter.target_spec.supported_types = [tf.int8]
# These set the input and output tensors to uint8 (added in r2.3)
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
tflite_model = converter.convert()

with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:
  f.write(tflite_model)

"""##Run inference on TFLite(float32) model with dog.jpg
"https://github.com/pytorch/hub/raw/master/images/dog.jpg"
"""

# Download an example image from the pytorch website
import urllib
url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
try: urllib.URLopener().retrieve(url, filename)
except: urllib.request.urlretrieve(url, filename)

# sample execution (requires torchvision)
from PIL import Image
from torchvision import transforms
input_image = Image.open(filename)
preprocess = transforms.Compose([
    transforms.Resize(224),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(input_image)
print(input_tensor.shape)
#input_tensor = tf.reshape(input_tensor,[3,IMAGE_SIZE,IMAGE_SIZE])
#input_tensor = tf.cast(input_tensor , tf.float32)
input_tensor = tf.expand_dims(input_tensor, 0)
print("torch input_tensor size")
print(input_tensor.shape)

import numpy as np
import tensorflow as tf

tflite_model_path = './mobilenet_v2_float32.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data
input_shape = input_details[0]['shape']
print(input_shape)
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_tensor)

interpreter.invoke()

# get_tensor() returns a copy of the tensor data
# use tensor() in order to get a pointer to the tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

print("Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%"
      .format(np.argmax(output_data),
              100 * output_data[0][np.argmax(output_data)]))

"""##Run inference on TFLite(quantized) model with dog.jpg
"https://github.com/pytorch/hub/raw/master/images/dog.jpg"
"""

# Download an example image from the pytorch website
import urllib
url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
try: urllib.URLopener().retrieve(url, filename)
except: urllib.request.urlretrieve(url, filename)

import tensorflow as tf
import numpy as np
tflite_model_path = './mobilenet_v2_1.0_224_quant.tflite'
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
test_details = interpreter.get_input_details()[0]

scale, zero_point = test_details['quantization']
print(scale)
print(zero_point)

# Test the model on image  data
# sample execution (requires torchvision)
from PIL import Image
from torchvision import transforms
input_image = Image.open(filename)
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(input_image)
print(input_tensor.shape)
input_tensor = torch.unsqueeze(input_tensor, 0)
input_tensor = torch.quantize_per_tensor(input_tensor, torch.tensor(scale), torch.tensor(zero_point), torch.qint8)
input_tensor = torch.int_repr(input_tensor).numpy()

print("torch input_tensor size:")
print(input_tensor.shape)
print(input_tensor)
interpreter.set_tensor(input_details[0]['index'], input_tensor)

interpreter.invoke()

# get_tensor() returns a copy of the tensor data
# use tensor() in order to get a pointer to the tensor
output_data = interpreter.get_tensor(output_details[0]['index'])
print("Predicted value . Label index: {}, confidence: {:2.0f}%"
      .format(np.argmax(output_data),
              100 * output_data[0][np.argmax(output_data)]))

